# -*- coding: utf-8 -*-
"""Gargaro_David_845738_assignment2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12PWGGK18-byz27t1_PQ36HG_-YubDZZP

# Seeding and Dataset
"""

# import dependencies
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt

# set seed
keras.utils.set_random_seed(42)

# load the data and divide it into train/test split
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

print(x_train.shape, y_train.shape)
# check shape

"""We now normalize the images of the dataset so that their range is between 0 and 1"""

# Scale images into the [0 1] range
x_train = x_train.astype('float32') / 255
x_test = x_test.astype('float32') / 255

# Make images with size (28,28,1)
x_train = np.expand_dims(x_train, -1)
x_test = np.expand_dims(x_test, -1)

"""For multi-class problems, we need to convert the label representing the class into a one-hot vector."""

# convert class array into one-hot representation
num_classes = 10
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

"""# Models and Training"""

# Model/data parameters
model = keras.Sequential([
    keras.Input(shape=(28,28,1)),
    keras.layers.Conv2D(10, kernel_size=(4, 4), activation='relu'),
    keras.layers.MaxPool2D(pool_size=(5, 5)),
    keras.layers.Conv2D(17, kernel_size=(2, 2), activation='relu'),
    keras.layers.MaxPool2D(pool_size=(2, 2)),
    keras.layers.Flatten(),
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dense(num_classes, activation='softmax')
])

model.summary()

# compile
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# train
history = model.fit(x_train, y_train, batch_size=128, epochs=20, validation_split=0.2)

"""# Results"""

def plot_performance(history):
  fig, ax = plt.subplots(1, 2)
  fig.tight_layout()
  train_acc = history.history['accuracy']
  valid_acc = history.history['val_accuracy']
  train_loss = history.history['loss']
  valid_loss = history.history['val_loss']
  ax[0].set_xlabel('Epoch')
  ax[0].set_ylabel('Loss')
  ax[0].set_title('Loss')
  ax[0].plot(train_loss)
  ax[0].plot(valid_loss)
  ax[1].set_title('Accuracy')
  ax[1].set_xlabel('Epoch')
  ax[1].set_ylabel('Accuracy')
  ax[1].plot(train_acc)
  ax[1].plot(valid_acc)
  plt.show()

plot_performance(history)

# evaluate performance
scores = model.evaluate(x_test, y_test, verbose=0)
print("Loss:", scores[0])
print("Accuracy:", scores[1])

"""### Conclusion

# Model
The designed neural network is a convolutional neural network (CNN) for image classification.
Here's a breakdown of the architecture and design choices:

* Input Layer (Input shape: 28x28x1) that accepts grayscale images of size 28x28 pixels.

* Convolutional Layer 1 (Conv2D - 10 filters, kernel size: 4x4, ReLU activation) applies 10 filters of size 4x4 to the input images, using the ReLU activation function.

* MaxPooling Layer 1 (Pool size: 5x5) with a pool size of 5x5 is applied to reduce the spatial dimensions, capturing the most important information from the previous convolutional layer.


* Convolutional Layer 2 (Conv2D - 17 filters, kernel size: 2x2, ReLU activation) with 17 filters of size 2x2 is applied, further capturing more complex patterns in the image.

* MaxPooling Layer 2 (Pool size: 2x2) with a pool size of 2x2 is applied to further reduce the spatial dimensions.

* Flatten Layer that makes the 2D matrix data into a vector, preparing it for input into the dense layers.

* Dense Layer 1 (64 neurons, ReLU activation) with 64 neurons and ReLU activation function, which learns intricate patterns from the flattened features.

* Output Layer (num_classes neurons, Softmax activation) with neurons equal to the number of classes in the dataset, using the softmax activation function for multi-class classification.

Convolutional Layer 1:
* Parameters = (4 * 4 * 1 * 10) + 10 (biases) = 170

* MaxPooling Layer 1:
No parameters (pooling layers have no trainable parameters)

* Convolutional Layer 2:
Parameters = (2 * 2 * 10 * 17) + 17 (biases) = 697

* MaxPooling Layer 2:
No parameters

* Flatten Layer:
No parameters

* Dense Layer 1:
Parameters = 4416

* Output Layer:
Parameters = 650

Total Parameters = 170 + 697 + 4416 + 650 = 5933

Classification performance metrics used are Loss and Accuracy
The results is:
* Loss: 0.0489
* Accuracy: 98.35%

The low loss value indicates that the model's predictions are close to the actual values, and the high accuracy of 98.35% suggests that it correctly classifies the images
"""